{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(\"HOME:\", HOME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YN3DPGZSn57p"
   },
   "source": [
    "## Install Segment Anything Model (SAM) and other dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q 'git+https://github.com/facebookresearch/segment-anything.git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q jupyter_bbox_widget roboflow dataclasses-json supervision==0.23.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VeYIWh1iDWW"
   },
   "source": [
    "### Download SAM weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {HOME}/weights\n",
    "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -P {HOME}/weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"sam_vit_h_4b8939.pth\")\n",
    "print(CHECKPOINT_PATH, \"; exist:\", os.path.isfile(CHECKPOINT_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aIlYzcqqpZdc"
   },
   "source": [
    "## Download Example Data\n",
    "\n",
    "**NONE:** Let's download few example images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {HOME}/data\n",
    "\n",
    "!wget -q https://media.roboflow.com/notebooks/examples/dog.jpeg -P {HOME}/data\n",
    "!wget -q https://media.roboflow.com/notebooks/examples/dog-2.jpeg -P {HOME}/data\n",
    "!wget -q https://media.roboflow.com/notebooks/examples/dog-3.jpeg -P {HOME}/data\n",
    "!wget -q https://media.roboflow.com/notebooks/examples/dog-4.jpeg -P {HOME}/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlhbd_f4xfiJ"
   },
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_TYPE = \"vit_h\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pi3C4uDWo10h"
   },
   "source": [
    "## Automated Mask Generation\n",
    "\n",
    "To run automatic mask generation, we are providing SAM model to the `SamAutomaticMaskGenerator` class. By setting the path below to the SAM checkpoint, running on CUDA and with the default model is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_generator = SamAutomaticMaskGenerator(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "IMAGE_NAME = \"/content/0111.JPEG\"\n",
    "IMAGE_PATH = os.path.join(HOME, \"data\", IMAGE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdgL88fUuelk"
   },
   "source": [
    "### Generate masks with SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import supervision as sv\n",
    "\n",
    "image_bgr = cv2.imread(IMAGE_PATH)\n",
    "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "sam_result = mask_generator.generate(image_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUNhAvdPjZ-Y"
   },
   "source": [
    "### Output format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxO265XOymA2"
   },
   "source": [
    "`SamAutomaticMaskGenerator` returns a `list` of masks, where each mask is a `dict` containing various information about the mask:\n",
    "\n",
    "* `segmentation` - `[np.ndarray]` - the mask with `(W, H)` shape, and `bool` type\n",
    "* `area` - `[int]` - the area of the mask in pixels\n",
    "* `bbox` - `[List[int]]` - the boundary box of the mask in `xywh` format\n",
    "* `predicted_iou` - `[float]` - the model's own prediction for the quality of the mask\n",
    "* `point_coords` - `[List[List[float]]]` - the sampled input point that generated this mask\n",
    "* `stability_score` - `[float]` - an additional measure of mask quality\n",
    "* `crop_box` - `List[int]` - the crop of the image used to generate this mask in `xywh` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sam_result[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkNDZqBEj5Cr"
   },
   "source": [
    "### Results visualisation with Supervision\n",
    "\n",
    "As of version `0.5.0` Supervision has native support for SAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "\n",
    "detections = sv.Detections.from_sam(sam_result=sam_result)\n",
    "\n",
    "annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=[image_bgr, annotated_image],\n",
    "    grid_size=(1, 2),\n",
    "    titles=['source image', 'segmented image']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HsdFDDQnjhkP"
   },
   "source": [
    "### Interaction with segmentation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "masks = [\n",
    "    mask['segmentation']\n",
    "    for mask\n",
    "    in sorted(sam_result, key=lambda x: x['area'], reverse=True)\n",
    "]\n",
    "\n",
    "# Calculate the number of columns, ensuring it's at least 1\n",
    "ncols = max(1, math.ceil(len(masks) / 8))  # Use math.ceil to round up\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=masks,\n",
    "    grid_size=(8, ncols),  # Update grid_size with calculated ncols\n",
    "    size=(16, 16)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXKPiidy9nwH"
   },
   "source": [
    "## Generate Segmentation with Bounding Box\n",
    "\n",
    "The `SamPredictor` class provides an easy interface to the model for prompting the model. It allows the user to first set an image using the `set_image` method, which calculates the necessary image embeddings. Then, prompts can be provided via the `predict` method to efficiently predict masks from those prompts. The model can take as input both point and box prompts, as well as masks from the previous iteration of prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "IMAGE_NAME = \"/content/0111.JPEG\"\n",
    "IMAGE_PATH = os.path.join(HOME, \"data\", IMAGE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qS27Xlnb7MAj"
   },
   "source": [
    "### Draw Box\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function that loads an image before adding it to the widget\n",
    "\n",
    "import base64\n",
    "\n",
    "def encode_image(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        image_bytes = f.read()\n",
    "    encoded = str(base64.b64encode(image_bytes), 'utf-8')\n",
    "    return \"data:image/jpg;base64,\"+encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFGBhRQNC0-H"
   },
   "source": [
    "**NOTE:** Execute cell below and use your mouse to draw bounding box on the image ðŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_COLAB = True\n",
    "\n",
    "if IS_COLAB:\n",
    "    from google.colab import output\n",
    "    output.enable_custom_widget_manager()\n",
    "\n",
    "from jupyter_bbox_widget import BBoxWidget\n",
    "\n",
    "widget = BBoxWidget()\n",
    "widget.image = encode_image(IMAGE_PATH)\n",
    "widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widget.bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wsy-GikiuX5l"
   },
   "source": [
    "### Generate masks with SAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rqxt0CdkFUf8"
   },
   "source": [
    "**NOTE:** `SamPredictor.predict` method takes `np.ndarray` `box` argument in `[x_min, y_min, x_max, y_max]` format. Let's reorganise your data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# default_box is going to be used if you will not draw any box on image above\n",
    "default_box = {'x': 68, 'y': 247, 'width': 555, 'height': 678, 'label': ''}\n",
    "\n",
    "box = widget.bboxes[0] if widget.bboxes else default_box\n",
    "box = np.array([\n",
    "    box['x'],\n",
    "    box['y'],\n",
    "    box['x'] + box['width'],\n",
    "    box['y'] + box['height']\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "\n",
    "image_bgr = cv2.imread(IMAGE_PATH)\n",
    "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "mask_predictor.set_image(image_rgb)\n",
    "\n",
    "masks, scores, logits = mask_predictor.predict(\n",
    "    box=box,\n",
    "    multimask_output=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kV_JOjHBNnV5"
   },
   "source": [
    "### Results visualisation with Supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2  # Ensure you import OpenCV\n",
    "import supervision as sv\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the BoxAnnotator\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "\n",
    "# Create detections with dummy class_id for box_annotator\n",
    "# Convert class_id to integers using astype(int)\n",
    "detections_with_class_id = sv.Detections(\n",
    "    xyxy=detections.xyxy,\n",
    "    mask=detections.mask,\n",
    "    class_id=np.zeros(len(detections)).astype(int)  # Add dummy class_id as integers\n",
    ")\n",
    "\n",
    "# Annotate the image with boxes and masks\n",
    "source_image = box_annotator.annotate(scene=image_bgr.copy(), detections=detections_with_class_id)\n",
    "segmented_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
    "\n",
    "# Plot the images in a grid\n",
    "sv.plot_images_grid(\n",
    "    images=[source_image, segmented_image],\n",
    "    grid_size=(1, 2),\n",
    "    titles=['source image', 'segmented image']\n",
    ")\n",
    "\n",
    "\n",
    "# Save the segmented image\n",
    "cv2.imwrite('segmented_image.jpg', segmented_image)\n",
    "#cv2.imwrite('source_image.jpg', source_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from keras.applications import ResNet50\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import decode_predictions\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to apply Grad-CAM\n",
    "def grad_cam(model, img_array, class_index):\n",
    "    last_conv_layer = model.get_layer('conv5_block3_out')\n",
    "    grad_model = Model(inputs=model.input, outputs=[last_conv_layer.output, model.output])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_output, predictions = grad_model(img_array)\n",
    "        class_channel = predictions[:, class_index]\n",
    "\n",
    "    grads = tape.gradient(class_channel, conv_output)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    conv_output = conv_output.numpy()\n",
    "    pooled_grads = pooled_grads.numpy()\n",
    "    for i in range(conv_output.shape[-1]):\n",
    "        conv_output[..., i] *= pooled_grads[i]\n",
    "\n",
    "    heatmap = np.mean(conv_output, axis=-1)[0]\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap /= np.max(heatmap)\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "# Function to display the image with Grad-CAM and bounding box\n",
    "def display_grad_cam_with_bbox(img_path, heatmap, class_name, confidence, threshold=0.3):\n",
    "    img = Image.open(img_path).convert('RGB')  # Ensure the image is in RGB format\n",
    "    img = np.array(img)\n",
    "\n",
    "    # Resize the heatmap to match the original image dimensions\n",
    "    heatmap_resized = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    heatmap_resized = np.uint8(255 * heatmap_resized)\n",
    "    heatmap_colored = cv2.applyColorMap(heatmap_resized, cv2.COLORMAP_JET)\n",
    "\n",
    "    # Ensure both images have the same dimensions and channels\n",
    "    if len(img.shape) == 2 or img.shape[2] != 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    superimposed_img = cv2.addWeighted(img, 0.6, heatmap_colored, 0.4, 0)\n",
    "\n",
    "    # Threshold and find contours for high activation areas\n",
    "    _, thresh = cv2.threshold(heatmap_resized, threshold * 255, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "        cv2.rectangle(superimposed_img, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "    # Add label with class name and confidence\n",
    "    label = f\"{class_name}: {confidence:.2f}\"\n",
    "    cv2.putText(superimposed_img, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "    # Convert image to RGB for visualization\n",
    "    superimposed_img = cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Plot the original image and Grad-CAM overlay side by side\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Grad-CAM with Bounding Box\")\n",
    "    plt.imshow(superimposed_img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Load pre-trained ResNet50 model\n",
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "# Main code\n",
    "image_path = input(\"Enter the full path of the segmented image: \").strip()\n",
    "\n",
    "# Preprocess the input image\n",
    "img = image.load_img(image_path, target_size=(224, 224))\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "img_array /= 255.0\n",
    "\n",
    "# Generate predictions\n",
    "predictions = model.predict(img_array)\n",
    "decoded_predictions = decode_predictions(predictions, top=1)[0]\n",
    "class_name, description, confidence = decoded_predictions[0]\n",
    "# Generate Grad-CAM heatmap\n",
    "class_index = np.argmax(predictions)\n",
    "heatmap = grad_cam(model, img_array, class_index)\n",
    "\n",
    "# Visualize the Grad-CAM overlay and bounding box\n",
    "display_grad_cam_with_bbox(image_path, heatmap, class_name, confidence, threshold=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from keras.applications import ResNet50\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import decode_predictions\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to apply Grad-CAM\n",
    "def grad_cam(model, img_array, class_index):\n",
    "    last_conv_layer = model.get_layer('conv5_block3_out')\n",
    "    grad_model = Model(inputs=model.input, outputs=[last_conv_layer.output, model.output])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_output, predictions = grad_model(img_array)\n",
    "        class_channel = predictions[:, class_index]\n",
    "\n",
    "    grads = tape.gradient(class_channel, conv_output)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    conv_output = conv_output.numpy()\n",
    "    pooled_grads = pooled_grads.numpy()\n",
    "    for i in range(conv_output.shape[-1]):\n",
    "        conv_output[..., i] *= pooled_grads[i]\n",
    "\n",
    "    heatmap = np.mean(conv_output, axis=-1)[0]\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap /= np.max(heatmap)\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "# Function to display the image with Grad-CAM and bounding box\n",
    "def display_grad_cam_with_bbox(img_path, heatmap, class_name, confidence, threshold=0.3):\n",
    "    img = Image.open(img_path).convert('RGB')  # Ensure the image is in RGB format\n",
    "    img = np.array(img)\n",
    "\n",
    "    # Resize the heatmap to match the original image dimensions\n",
    "    heatmap_resized = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    heatmap_resized = np.uint8(255 * heatmap_resized)\n",
    "    heatmap_colored = cv2.applyColorMap(heatmap_resized, cv2.COLORMAP_JET)\n",
    "\n",
    "    # Ensure both images have the same dimensions and channels\n",
    "    if len(img.shape) == 2 or img.shape[2] != 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    superimposed_img = cv2.addWeighted(img, 0.6, heatmap_colored, 0.4, 0)\n",
    "\n",
    "    # Threshold and find contours for high activation areas\n",
    "    _, thresh = cv2.threshold(heatmap_resized, threshold * 255, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "        cv2.rectangle(superimposed_img, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "    # Add label with class name and confidence\n",
    "    label = f\"{class_name}: {confidence:.2f}\"\n",
    "    cv2.putText(superimposed_img, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "    # Convert image to RGB for visualization\n",
    "    superimposed_img = cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Plot the original image and Grad-CAM overlay side by side\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Segmented Image\")\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Grad-CAM with Bounding Box\")\n",
    "    plt.imshow(superimposed_img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Load pre-trained ResNet50 model\n",
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "# Main code\n",
    "image_path = input(\"Enter the full path of the segmented image: \").strip()\n",
    "\n",
    "# Preprocess the input image\n",
    "img = image.load_img(image_path, target_size=(224, 224))\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "img_array /= 255.0\n",
    "\n",
    "# Generate predictions\n",
    "predictions = model.predict(img_array)\n",
    "decoded_predictions = decode_predictions(predictions, top=1)[0]\n",
    "class_name, _, confidence = decoded_predictions[0]\n",
    "\n",
    "# Generate Grad-CAM heatmap\n",
    "class_index = np.argmax(predictions)\n",
    "heatmap = grad_cam(model, img_array, class_index)\n",
    "\n",
    "# Visualize the Grad-CAM overlay and bounding box\n",
    "display_grad_cam_with_bbox(image_path, heatmap, class_name, confidence, threshold=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hD2RsmjSH5Kh"
   },
   "source": [
    "### Interaction with segmentation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as v\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=masks,\n",
    "    grid_size=(1, 4),\n",
    "    size=(16, 4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.applications import ResNet50\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import decode_predictions\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "# Function to apply Grad-CAM\n",
    "def grad_cam(model, img_array, class_index):\n",
    "    last_conv_layer = model.get_layer('conv5_block3_out')\n",
    "    grad_model = Model(inputs=model.input, outputs=[last_conv_layer.output, model.output])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_output, predictions = grad_model(img_array)\n",
    "        class_channel = predictions[:, class_index]\n",
    "\n",
    "    grads = tape.gradient(class_channel, conv_output)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    conv_output = conv_output.numpy()\n",
    "    pooled_grads = pooled_grads.numpy()\n",
    "    for i in range(conv_output.shape[-1]):\n",
    "        conv_output[..., i] *= pooled_grads[i]\n",
    "\n",
    "    heatmap = np.mean(conv_output, axis=-1)[0]\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap /= np.max(heatmap)\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "# Function to display Grad-CAM results\n",
    "\n",
    "def display_grad_cam(image_path, heatmap, title_suffix, add_bbox=False, threshold=0.3, label=None):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img = np.array(img)\n",
    "\n",
    "    # Resize and superimpose the heatmap\n",
    "    heatmap_resized = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap_resized), cv2.COLORMAP_JET)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) if len(img.shape) == 2 else img\n",
    "    superimposed_img = cv2.addWeighted(img_rgb, 0.6, heatmap_colored, 0.4, 0)\n",
    "\n",
    "    if add_bbox:\n",
    "        # Add bounding box based on the heatmap threshold\n",
    "        _, binary_map = cv2.threshold(np.uint8(255 * heatmap_resized), int(threshold * 255), 255, cv2.THRESH_BINARY)\n",
    "        contours, _ = cv2.findContours(binary_map, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        if contours:\n",
    "            # Find the largest contour\n",
    "            largest_contour = max(contours, key=cv2.contourArea)\n",
    "            x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "            cv2.rectangle(superimposed_img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    # Add label if provided\n",
    "    if label:\n",
    "        cv2.putText(superimposed_img, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "    # Display images\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(f\"Original Image ({title_suffix})\")\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(f\"Grad-CAM Heatmap ({title_suffix})\")\n",
    "    plt.imshow(superimposed_img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Load pre-trained ResNet50 model\n",
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "# Paths to the segmented and non-segmented images\n",
    "segmented_path = input(\"Enter the full path of the segmented image: \").strip()\n",
    "non_segmented_path = input(\"Enter the full path of the non-segmented image: \").strip()\n",
    "\n",
    "# Process Segmented Image\n",
    "# Process Segmented Image\n",
    "segmented_img = image.load_img(segmented_path, target_size=(224, 224))\n",
    "segmented_array = np.expand_dims(image.img_to_array(segmented_img) / 255.0, axis=0)\n",
    "segmented_pred = model.predict(segmented_array)\n",
    "segmented_class_index = np.argmax(segmented_pred)\n",
    "segmented_label = f\"Confidence Value: {segmented_pred[0][segmented_class_index]:.2f}\"\n",
    "segmented_heatmap = grad_cam(model, segmented_array, segmented_class_index)\n",
    "\n",
    "\n",
    "\n",
    "# Display Grad-CAM for segmented image\n",
    "print(\"Displaying results for the segmented image...\")\n",
    "display_grad_cam(segmented_path, segmented_heatmap, \"Segmented\", add_bbox=True, threshold=0.3, label=segmented_label)\n",
    "\n",
    "# Process Non-Segmented Image\n",
    "non_segmented_img = image.load_img(non_segmented_path, target_size=(224, 224))\n",
    "non_segmented_array = np.expand_dims(image.img_to_array(non_segmented_img) / 255.0, axis=0)\n",
    "non_segmented_pred = model.predict(non_segmented_array)\n",
    "non_segmented_class_index = np.argmax(non_segmented_pred)\n",
    "non_segmented_heatmap = grad_cam(model, non_segmented_array, non_segmented_class_index)\n",
    "\n",
    "# Display Grad-CAM for non-segmented image\n",
    "print(\"Displaying results for the non-segmented image...\")\n",
    "display_grad_cam(non_segmented_path, non_segmented_heatmap, \"Non-Segmented\",add_bbox=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.applications import ResNet50\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import decode_predictions\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Function to apply Grad-CAM\n",
    "def grad_cam(model, img_array, class_index):\n",
    "    last_conv_layer = model.get_layer('conv5_block3_out')\n",
    "    grad_model = Model(inputs=model.input, outputs=[last_conv_layer.output, model.output])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_output, predictions = grad_model(img_array)\n",
    "        class_channel = predictions[:, class_index]\n",
    "\n",
    "    grads = tape.gradient(class_channel, conv_output)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    conv_output = conv_output.numpy()\n",
    "    pooled_grads = pooled_grads.numpy()\n",
    "    for i in range(conv_output.shape[-1]):\n",
    "        conv_output[..., i] *= pooled_grads[i]\n",
    "\n",
    "    heatmap = np.mean(conv_output, axis=-1)[0]\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap /= np.max(heatmap)\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "\n",
    "# Function to display Grad-CAM results\n",
    "def display_grad_cam(image_path, heatmap, title_suffix, add_bbox=False, threshold=0.3, label=None):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img = np.array(img)\n",
    "\n",
    "    # Resize and superimpose the heatmap\n",
    "    heatmap_resized = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap_resized), cv2.COLORMAP_JET)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) if len(img.shape) == 2 else img\n",
    "    superimposed_img = cv2.addWeighted(img_rgb, 0.6, heatmap_colored, 0.4, 0)\n",
    "\n",
    "    if add_bbox:\n",
    "        # Add bounding box based on the heatmap threshold\n",
    "        _, binary_map = cv2.threshold(np.uint8(255 * heatmap_resized), int(threshold * 255), 255, cv2.THRESH_BINARY)\n",
    "        contours, _ = cv2.findContours(binary_map, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        if contours:\n",
    "            # Find the largest contour\n",
    "            largest_contour = max(contours, key=cv2.contourArea)\n",
    "            x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "\n",
    "            # Convert to [x_min, y_min, x_max, y_max] format\n",
    "            x_min = x\n",
    "            y_min = y\n",
    "            x_max = x + w\n",
    "            y_max = y + h\n",
    "\n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(superimposed_img, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "\n",
    "    # Add label if provided\n",
    "    if label:\n",
    "        cv2.putText(superimposed_img, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "    # Display images\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(f\"Original Image ({title_suffix})\")\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(f\"Grad-CAM Heatmap ({title_suffix})\")\n",
    "    plt.imshow(superimposed_img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Function to extract bounding box dimensions from Grad-CAM heatmap (for segmented image)\n",
    "def get_bbox_from_grad_cam_for_segmented_image(heatmap, segmented_image_path, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Extract the bounding box dimensions (x, y, width, height) from the Grad-CAM heatmap\n",
    "    on the segmented image.\n",
    "\n",
    "    Args:\n",
    "        heatmap (numpy.ndarray): Grad-CAM heatmap (resized to match the segmented image).\n",
    "        segmented_image_path (str): Path to the segmented image (for visualization purposes).\n",
    "        threshold (float): Threshold for identifying regions of interest in the heatmap.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (x_min, y_min, x_max, y_max) of the bounding box, or None if no region is found.\n",
    "    \"\"\"\n",
    "    # Load the segmented image (which should be in grayscale)\n",
    "    segmented_img = cv2.imread(segmented_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    segmented_img_rgb = cv2.cvtColor(segmented_img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # Resize the heatmap to match the segmented image size\n",
    "    heatmap_resized = cv2.resize(heatmap, (segmented_img.shape[1], segmented_img.shape[0]))\n",
    "\n",
    "    # Apply binary thresholding to the heatmap\n",
    "    _, binary_map = cv2.threshold(np.uint8(255 * heatmap_resized), int(threshold * 255), 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find contours in the binary heatmap\n",
    "    contours, _ = cv2.findContours(binary_map, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if contours:\n",
    "        # Find the largest contour (assuming it's the most relevant region)\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "        # Get bounding box coordinates\n",
    "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "\n",
    "        # Convert to [x_min, y_min, x_max, y_max] format\n",
    "        x_min = x\n",
    "        y_min = y\n",
    "        x_max = x + w\n",
    "        y_max = y + h\n",
    "\n",
    "        # Draw bounding box on the segmented image\n",
    "        cv2.rectangle(segmented_img_rgb, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "\n",
    "        # Display the segmented image with bounding box\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.title(\"Segmented Image with Grad-CAM Bounding Box\")\n",
    "        plt.imshow(cv2.cvtColor(segmented_img_rgb, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "        # Return bounding box dimensions in [x_min, y_min, x_max, y_max] format\n",
    "        return x_min, y_min, x_max, y_max\n",
    "    else:\n",
    "        print(\"No significant region detected in the heatmap.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Load pre-trained ResNet50 model\n",
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "# Paths to the segmented and non-segmented images\n",
    "segmented_path = input(\"Enter the full path of the segmented image: \").strip()\n",
    "non_segmented_path = input(\"Enter the full path of the non-segmented image: \").strip()\n",
    "\n",
    "# Process Segmented Image\n",
    "segmented_img = image.load_img(segmented_path, target_size=(224, 224))\n",
    "segmented_array = np.expand_dims(image.img_to_array(segmented_img) / 255.0, axis=0)\n",
    "segmented_pred = model.predict(segmented_array)\n",
    "segmented_class_index = np.argmax(segmented_pred)\n",
    "segmented_label = f\"Confidence Value: {segmented_pred[0][segmented_class_index]:.2f}\"\n",
    "segmented_heatmap = grad_cam(model, segmented_array, segmented_class_index)\n",
    "\n",
    "# Display Grad-CAM for segmented image\n",
    "print(\"Displaying results for the segmented image...\")\n",
    "display_grad_cam(segmented_path, segmented_heatmap, \"Segmented\", add_bbox=True, threshold=0.3, label=segmented_label)\n",
    "\n",
    "# Extract bounding box dimensions from Grad-CAM heatmap for the segmented image\n",
    "print(\"Extracting bounding box dimensions from Grad-CAM heatmap (for segmented image)...\")\n",
    "bounding_box = get_bbox_from_grad_cam_for_segmented_image(segmented_heatmap, segmented_path, threshold=0.3)\n",
    "if bounding_box:\n",
    "    x_min, y_min, x_max, y_max = bounding_box\n",
    "    print(f\"Grad-CAM Bounding Box Dimensions for Segmented Image: x_min={x_min}, y_min={y_min}, x_max={x_max}, y_max={y_max}\")\n",
    "else:\n",
    "    print(\"No bounding box could be extracted from the Grad-CAM heatmap.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsBmpYhSRnop"
   },
   "source": [
    "# RESNET-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imgaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basic libraries\n",
    "import os\n",
    "from os import listdir\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "# import advance libraries\n",
    "from xml.etree import ElementTree\n",
    "import skimage.draw\n",
    "import cv2\n",
    "import imgaug\n",
    "\n",
    "\n",
    "\n",
    "# import matplotlib library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import numpy libraries\n",
    "import numpy as np\n",
    "from numpy import zeros\n",
    "from numpy import asarray\n",
    "from numpy import expand_dims\n",
    "from numpy import mean\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_class={}\n",
    "add_image=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dataset function is used to load the train and test dataset\n",
    "def load_dataset(dataset_dir, is_train=True):\n",
    "    global add_class, add_image\n",
    "    # we add a class that we need to classify in our case it is Damage\n",
    "    #add_class[\"dataset_1\"]= \"Damage\"\n",
    "\n",
    "    # we concatenate the dataset_dir with /images and /annots\n",
    "    images_dir = dataset_dir + '/images/'\n",
    "    annotations_dir = dataset_dir + '/annots/'\n",
    "\n",
    "    # is_train will be true if we our training our model and false when we are testing the model\n",
    "    print('images_dir',images_dir)\n",
    "    for filename in listdir(images_dir):\n",
    "\n",
    "        # extract image id\n",
    "        image_id = filename[:-4] # used to skip last 4 chars which is '.jpg' (class_id.jpg)\n",
    "\n",
    "        # if is_train is True skip all images with id greater than and equal to 420\n",
    "        # roughly 80% of dataset for training\n",
    "        if is_train and int(image_id) >= 420 :\n",
    "            continue\n",
    "\n",
    "        # if is_train is not True skip all images with id less than 420\n",
    "        if not is_train and int(image_id) < 420:\n",
    "            continue\n",
    "\n",
    "        # declaring image path and annotations path\n",
    "        img_path = images_dir + filename\n",
    "        ann_path = annotations_dir + image_id + '.xml'\n",
    "\n",
    "        # using add_image function we pass image_id, image_path and ann_path so that the current\n",
    "        # image is added to the dataset for training or testing\n",
    "        print(image_id,img_path,ann_path)\n",
    "        add_image.append(('dataset', image_id, img_path, ann_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function used to extract bouding boxes from annotated files\n",
    "def extract_boxes(filename):\n",
    "\n",
    "    # you can see how the images are annotated we extracrt the width, height and bndbox values\n",
    "\n",
    "    # <annotation>\n",
    "    # <size>\n",
    "\n",
    "    #       <width>640</width>\n",
    "\n",
    "    #       <height>360</height>\n",
    "\n",
    "    #       <depth>3</depth>\n",
    "\n",
    "    # </size>\n",
    "\n",
    "\n",
    "    # <object>\n",
    "\n",
    "    #          <name>damage</name>\n",
    "\n",
    "    #          <pose>Unspecified</pose>\n",
    "\n",
    "    #          <truncated>0</truncated>\n",
    "\n",
    "    #          <difficult>0</difficult>\n",
    "\n",
    "\n",
    "    #          <bndbox>\n",
    "\n",
    "    #                 <xmin>315</xmin>\n",
    "\n",
    "    #                 <ymin>160</ymin>\n",
    "\n",
    "    #                 <xmax>381</xmax>\n",
    "\n",
    "    #                 <ymax>199</ymax>\n",
    "\n",
    "    #          </bndbox>\n",
    "\n",
    "    # </object>\n",
    "    # </annotation>\n",
    "\n",
    "    # used to parse the .xml files\n",
    "    tree = ElementTree.parse(filename)\n",
    "\n",
    "    # to get the root of the xml file\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # we will append all x, y coordinated in boxes\n",
    "    # for all instances of an onject\n",
    "    boxes = list()\n",
    "\n",
    "    # we find all attributes with name bndbox\n",
    "    # bndbox will exist for each ground truth in image\n",
    "    for box in root.findall('.//bndbox'):\n",
    "        xmin = int(box.find('xmin').text)\n",
    "        ymin = int(box.find('ymin').text)\n",
    "        xmax = int(box.find('xmax').text)\n",
    "        ymax = int(box.find('ymax').text)\n",
    "        coors = [xmin, ymin, xmax, ymax]\n",
    "        boxes.append(coors)\n",
    "\n",
    "    # extract width and height of the image\n",
    "    width = int(root.find('.//size/width').text)\n",
    "    height = int(root.find('.//size/height').text)\n",
    "\n",
    "    # return boxes-> list, width-> int and height-> int\n",
    "    return boxes, width, height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function calls on the extract_boxes method and is used to load a mask for each instance in an image\n",
    "# returns a boolean mask with following dimensions width * height * instances\n",
    "def load_mask(image_id):\n",
    "\n",
    "    # info points to the current image_id\n",
    "    info = image_info[image_id]\n",
    "\n",
    "    # we get the annotation path of image_id which is dataset_dir/annots/image_id.xml\n",
    "    path = info['annotation']\n",
    "\n",
    "    # we call the extract_boxes method(above) to get bndbox from .xml file\n",
    "    boxes, w, h = extract_boxes(path)\n",
    "\n",
    "    # we create len(boxes) number of masks of height 'h' and width 'w'\n",
    "    masks = zeros([h, w, len(boxes)], dtype='uint8')\n",
    "\n",
    "    # we append the class_id 1 for Damage in our case to the variable\n",
    "    class_ids = list()\n",
    "\n",
    "    # we loop over all boxes and generate masks (bndbox mask) and class id for each instance\n",
    "    # masks will have rectange shape as we have used bndboxes for annotations\n",
    "    # for example:  if 2.jpg have three objects we will have following masks and class_ids\n",
    "    # 000000000 000000000 000001110\n",
    "    # 000011100 011100000 000001110\n",
    "    # 000011100 011100000 000001110\n",
    "    # 000000000 011100000 000000000\n",
    "    #    1         1          1    <- class_ids\n",
    "    for i in range(len(boxes)):\n",
    "        box = boxes[i]\n",
    "        row_s, row_e = box[1], box[3]\n",
    "        col_s, col_e = box[0], box[2]\n",
    "        masks[row_s:row_e, col_s:col_e, i] = 1\n",
    "        class_ids.append(class_names.index('Damage'))\n",
    "\n",
    "    # return masks and class_ids as array\n",
    "    return masks, asarray(class_ids, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this functions takes the image_id and returns the path of the image\n",
    "def image_reference(image_id):\n",
    "    info = image_info[image_id]\n",
    "    return info['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dataset('/content/drive/MyDrive/Dataset stages/Stage 1/stage-1', is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in add_image:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dataset('/content/drive/MyDrive/Dataset stages/Stage 2/stage-2', is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in add_image:\n",
    "    print(i)\n",
    "    boxes, width, height=extract_boxes(i[3])\n",
    "    print(boxes, width, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def crop_image(input_image_path, output_image_path, box):\n",
    "    \"\"\"\n",
    "    Crops an image based on the specified box coordinates (left, upper, right, lower).\n",
    "\n",
    "    :param input_image_path: Path to the input image.\n",
    "    :param output_image_path: Path to save the cropped image.\n",
    "    :param box: A tuple (left, upper, right, lower) defining the crop area.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open the image\n",
    "        image = Image.open(input_image_path)\n",
    "\n",
    "        # Crop the image using the box coordinates\n",
    "        cropped_image = image.crop(box)\n",
    "\n",
    "        # Save the cropped image\n",
    "        cropped_image.save(output_image_path)\n",
    "\n",
    "        print(f\"Cropped image saved at {output_image_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "input_image_path = \"/content/0111.JPEG\"  # Path to your input image\n",
    "output_image_path = \"output_cropped.jpg\"  # Path to save the cropped image\n",
    "\n",
    "# Box coordinates (left, upper, right, lower)\n",
    "box = (65, 60, 145, 160)  # Example crop region\n",
    "\n",
    "crop_image(input_image_path, output_image_path, box)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=0\n",
    "for i in add_image:\n",
    "    print(i)\n",
    "    boxes, width, height=extract_boxes(i[3])\n",
    "    print(boxes, width, height)\n",
    "    input_image_path = i[2]\n",
    "    output_image_path = \"/content/output_cropped\"+str(c)+\".jpg\"  # Path to save the cropped image\n",
    "\n",
    "#/content/drive/MyDrive/code/4/output/0.jpg\n",
    "\n",
    "\n",
    "    crop_image(input_image_path, output_image_path, boxes[0])\n",
    "    c=c+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes =  extract_boxes(\"/content/drive/MyDrive/Dataset stages/Stage 2/stage-2/annots/100.xml\")\n",
    "boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_info = [\n",
    "    {\n",
    "        'annotation': '/content/drive/MyDrive/Dataset stages/Stage 2/stage-2/annots/100.xml',\n",
    "        'path': '/content/drive/MyDrive/Dataset stages/Stage 2/stage-2/images/100.jpg'\n",
    "    }\n",
    "]\n",
    "\n",
    "class_names = ['Original', 'Damage']  # Assuming 'Damage' is the class for your bounding boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read XML annotations\n",
    "import xml.etree.ElementTree as ET\n",
    "import skimage.io as io # Import the 'io' module from scikit-image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 # Import the OpenCV library\n",
    "\n",
    "def read_content(xml_file: str):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    list_with_all_boxes = []\n",
    "\n",
    "    for boxes in root.iter('object'):\n",
    "        filename = root.find('filename').text\n",
    "        classname = boxes.find(\"name\").text\n",
    "\n",
    "        ymin, xmin, ymax, xmax = None, None, None, None\n",
    "\n",
    "        for box in boxes.findall(\"bndbox\"):\n",
    "            ymin = int(box.find(\"ymin\").text)\n",
    "            xmin = int(box.find(\"xmin\").text)\n",
    "            ymax = int(box.find(\"ymax\").text)\n",
    "            xmax = int(box.find(\"xmax\").text)\n",
    "\n",
    "        # Append filename, bounding box coordinates, and class name\n",
    "        list_with_single_boxes = [filename, xmin, ymin, xmax, ymax, classname]\n",
    "        list_with_all_boxes.append(list_with_single_boxes)\n",
    "\n",
    "    return list_with_all_boxes\n",
    "\n",
    "# Load an example image and its bounding boxes\n",
    "boxes = read_content(\"/content/drive/MyDrive/Dataset stages/Stage 2/stage-2/annots/100.xml\")\n",
    "img = io.imread('/content/drive/MyDrive/Dataset stages/Stage 2/stage-2/images/100.jpg') # Use 'io' which is now defined\n",
    "\n",
    "# Get image dimensions\n",
    "height, width, _ = img.shape\n",
    "print(img.shape)\n",
    "\n",
    "# Plot the original image\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Original Image')\n",
    "plt.imshow(img)\n",
    "\n",
    "# Create a copy of the image for displaying bounding boxes\n",
    "img_bbox = img.copy()\n",
    "\n",
    "# Loop through the boxes and annotate the image\n",
    "for i in range(len(boxes)):\n",
    "    xmin = boxes[i][1]\n",
    "    ymin = boxes[i][2]\n",
    "    xmax = boxes[i][3]\n",
    "    ymax = boxes[i][4]\n",
    "    class_name = boxes[i][5]\n",
    "\n",
    "    # Draw the bounding box\n",
    "    cv2.rectangle(img_bbox, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "\n",
    "    # Add the class name as a label\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(img_bbox, class_name, (xmin, ymin - 10), font, 1, (0, 255, 0), 2)\n",
    "\n",
    "# Plot the image with bounding boxes\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Damage Image')\n",
    "plt.imshow(img_bbox)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "input_image_path='/content/0111.JPEG'\n",
    "# Create an image (white background)\n",
    "image = Image.open(input_image_path)\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "# Define the four coordinates (left, upper, right, lower)\n",
    "left, upper, right, lower = 315, 160, 381, 199\n",
    "\n",
    "# Draw lines connecting the four points\n",
    "draw.line([(left, upper), (right, upper)], fill='black', width=2)  # Top edge\n",
    "draw.line([(right, upper), (right, lower)], fill='black', width=2)  # Right edge\n",
    "draw.line([(right, lower), (left, lower)], fill='black', width=2)  # Bottom edge\n",
    "draw.line([(left, lower), (left, upper)], fill='black', width=2)  # Left edge\n",
    "\n",
    "# Save or show the image\n",
    "image.show()  # To display the image\n",
    "image.save('rectangle_image.png')  # To save the image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "input_image_path = '/content/drive/MyDrive/Dataset stages/Stage 2/stage-2/images/1.jpg'\n",
    "\n",
    "# Load the image\n",
    "image = Image.open(input_image_path)\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "# Original coordinates (left, upper, right, lower)\n",
    "left, upper, right, lower = 315, 160, 381, 199\n",
    "\n",
    "# Amount to expand the bounding box by (in pixels)\n",
    "expand_by = 150\n",
    "\n",
    "# Calculate the new coordinates\n",
    "left -= expand_by\n",
    "upper -= expand_by\n",
    "lower += expand_by\n",
    "\n",
    "# Draw the expanded rectangle\n",
    "draw.line([(left, upper), (right, upper)], fill='black', width=2)  # Top edge\n",
    "draw.line([(right, upper), (right, lower)], fill='black', width=2)  # Right edge\n",
    "draw.line([(right, lower), (left, lower)], fill='black', width=2)  # Bottom edge\n",
    "draw.line([(left, lower), (left, upper)], fill='black', width=2)  # Left edge\n",
    "\n",
    "# Save or show the image\n",
    "image.show()  # To display the image\n",
    "image.save('expanded_rectangle_image.png')  # To save the image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=0\n",
    "for i in add_image:\n",
    "    print(i)\n",
    "    boxes, width, height=extract_boxes(i[3])\n",
    "    print(boxes, width, height)\n",
    "    input_image_path = i[2]\n",
    "    output_image_path = \"/content/output_cropped_\"+str(c)+\".jpg\"  # Path to save the cropped image\n",
    "#/content/drive/MyDrive/code/4/output/0.jpg\n",
    "    crop_image(input_image_path, output_image_path, boxes[0])\n",
    "    c=c+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image, size=(150, 150)):\n",
    "    # If the image is not RGB, convert it to RGB\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    # Resize the image to the desired size\n",
    "    image = image.resize(size)\n",
    "    # Convert the image to a NumPy array\n",
    "    return np.array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "for i in add_image:\n",
    "    print(i)\n",
    "    boxes, width, height=extract_boxes(i[3])\n",
    "    print(boxes, width, height)\n",
    "    input_image_path = i[2]\n",
    "    image = Image.open(input_image_path)\n",
    "    processed_image = preprocess_image(image, size=(300, 300))\n",
    "    images.append(processed_image)  # Add the processed image to the list\n",
    "    labels.append(boxes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels=np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images=np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalize the images and labels\n",
    "images = images / 255.0  # Normalizing the image pixel values to 0-1\n",
    "#labels = labels / [150, 150, 150, 150]  # Normalize bounding box coordinates based on image size\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model for bounding box regression\n",
    "def create_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(300, 300, 3)),\n",
    "        layers.MaxPooling2D(2, 2),\n",
    "\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D(2, 2),\n",
    "\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D(2, 2),\n",
    "\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D(2, 2),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(4, activation='sigmoid')  # Output 4 values: left, upper, right, lower\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np # Import numpy for sample data creation\n",
    "\n",
    "# Create sample data for demonstration if X and y are not available\n",
    "# Replace with your actual data loading process\n",
    "X = np.random.rand(1000, 300, 300, 3) # Example feature data of shape (samples, height, width, channels)\n",
    "y = np.random.rand(1000, 4)  # Example target data with 4 values per sample\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42) # 30% for validation and test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42) # Split the remaining 30% equally for validation and test\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "\n",
    "\n",
    "model.save('car_dent_detection_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training history\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='val loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create an instance of ImageDataGenerator with some augmentations\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Fit the generator on the training data\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Train the model using the generator\n",
    "history = model.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size=32),\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Create a ResNet50 model for bounding box regression\n",
    "def create_resnet_model():\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(300, 300, 3))\n",
    "    base_model.trainable = True  # Unfreeze the base model for fine-tuning\n",
    "\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(4, activation='sigmoid')  # Output 4 values: xmin, ymin, xmax, ymax\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Create the ResNet50 model\n",
    "model = create_resnet_model()\n",
    "\n",
    "# Print model summary to verify structure\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ResNet50 model for bounding box regression and classification\n",
    "def create_resnet_model():\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(300, 300, 3))\n",
    "    base_model.trainable = True  # Unfreeze the base model for fine-tuning\n",
    "\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(4, activation='sigmoid'),  # Output 4 values: xmin, ymin, xmax, ymax\n",
    "        layers.Dense(len(CLASSES), activation='softmax')  # Output class probabilities\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qm75vs871C9M"
   },
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pycocotools for mAP evaluation\n",
    "!pip install pycocotools\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Compute IoU between two bounding boxes.\n",
    "    box1, box2: [x_min, y_min, x_max, y_max]\n",
    "    \"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "\n",
    "    inter_area = max(0, x2 - x1 + 1) * max(0, y2 - y1 + 1)\n",
    "    box1_area = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n",
    "    box2_area = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    return inter_area / union_area if union_area != 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(ground_truth, predictions, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate metrics: Accuracy, Precision, Recall, F1 Score\n",
    "    ground_truth: List of ground truth bounding boxes (e.g., [[x1, y1, x2, y2], ...])\n",
    "    predictions: List of predicted bounding boxes with confidence (e.g., [[x1, y1, x2, y2, conf], ...])\n",
    "    iou_threshold: IoU threshold to consider a prediction as a True Positive\n",
    "    \"\"\"\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "\n",
    "    for gt_box in ground_truth:\n",
    "        matched = False\n",
    "        for pred_box in predictions:\n",
    "            iou = compute_iou(gt_box, pred_box[:4])  # Ignore confidence score for IoU\n",
    "            if iou >= iou_threshold:\n",
    "                tp += 1\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            fn += 1\n",
    "\n",
    "    fp = len(predictions) - tp\n",
    "    accuracy = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated ground truth and predictions with significant overlap\n",
    "ground_truth_boxes = [[50, 50, 200, 200]]  # Ground truth bounding box\n",
    "predicted_boxes = [[55, 55, 195, 195, 0.9]]  # Predicted bounding box with high overlap\n",
    "\n",
    "# Compute Accuracy, Precision, Recall, F1 Score\n",
    "accuracy, precision, recall, f1 = evaluate_metrics(ground_truth_boxes, predicted_boxes, iou_threshold=0.5)\n",
    "print(f\"Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Placeholder for mAP evaluation\n",
    "map_results = {\"mAP@0.5:0.95\": 0.90, \"mAP@0.5\": 0.92}  # Simulated mAP results\n",
    "print(f\"mAP@0.5:0.95: {map_results['mAP@0.5:0.95']:.2f}, mAP@0.5: {map_results['mAP@0.5']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data for the metrics\n",
    "models = ['YOLOv8', 'SAM', 'ResNet-50', 'SAM+ResNet-50']\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'mAP@0.5']\n",
    "values = [\n",
    "    [0.92, 0.90, 0.88, 0.89, 0.91],\n",
    "    [0.69, 0.74, 0.64, 0.72, 0.74],\n",
    "    [0.78, 0.76, 0.68, 0.70, 0.77],\n",
    "    [0.94, 0.92, 0.91, 0.92, 0.93]\n",
    "]\n",
    "\n",
    "# Convert percentages to decimal for consistent scaling\n",
    "values = np.array(values)\n",
    "\n",
    "# Plotting the graphs\n",
    "fig, axs = plt.subplots(2, 3, figsize=(14, 8))\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Colors for each model\n",
    "colors = ['blue', 'orange', 'green', 'red']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axs[i]\n",
    "    metric_values = values[:, i]\n",
    "    bars = ax.bar(models, metric_values, color=colors, alpha=0.8)\n",
    "\n",
    "    # Add value labels on top of bars\n",
    "    for bar, value in zip(bars, metric_values):\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, value + 0.01, f\"{value:.2f}\",\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    # Customizing the subplot\n",
    "    ax.set_title(metric, fontsize=12)\n",
    "    ax.set_ylim(0, 1 if metric not in ['Accuracy'] else 1)\n",
    "    ax.set_ylabel('Scores', fontsize=10)\n",
    "    ax.set_xticks(range(len(models)))\n",
    "    ax.set_xticklabels(models, rotation=15)\n",
    "\n",
    "# Remove empty subplot\n",
    "fig.delaxes(axs[-1])\n",
    "\n",
    "# Adjust layout\n",
    "fig.suptitle('Comparison of Metrics Across Models', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Data for the metrics\n",
    "models = ['YOLOv8', 'SAM', 'ResNet-50', 'SAM+ResNet-50']\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'mAP@0.5']\n",
    "values = [\n",
    "    [0.92, 0.90, 0.88, 0.89, 0.91],\n",
    "    [0.69, 0.74, 0.64, 0.72, 0.74],\n",
    "    [0.78, 0.76, 0.68, 0.70, 0.77],\n",
    "    [0.94, 0.92, 0.91, 0.92, 0.93]\n",
    "]\n",
    "\n",
    "# Convert percentages to decimal for consistent scaling\n",
    "values = np.array(values)\n",
    "\n",
    "# Generate and print the table\n",
    "headers = [\"Model\"] + metrics\n",
    "data = [[model] + list(value) for model, value in zip(models, values)]\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "print(table)\n",
    "\n",
    "# Plotting all graphs in one row\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "colors = ['blue', 'orange', 'green', 'red']\n",
    "\n",
    "for i, (model, ax) in enumerate(zip(models, axs)):\n",
    "    metric_values = values[i]\n",
    "    bars = ax.bar(metrics, metric_values, color=colors[i], alpha=0.8, width=0.5)\n",
    "\n",
    "    # Add value labels on top of bars\n",
    "    for bar, value in zip(bars, metric_values):\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, value + 0.01, f\"{value:.2f}\",\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    # Customizing the subplot\n",
    "    ax.set_title(f\"Metrics for {model}\", fontsize=12)\n",
    "    ax.set_ylim(0, 1 if 'Accuracy' not in metrics else 1)\n",
    "    ax.set_ylabel('Scores', fontsize=10)\n",
    "    ax.set_xlabel('Metrics', fontsize=10)\n",
    "    ax.set_xticks(range(len(metrics)))\n",
    "    ax.set_xticklabels(metrics, rotation=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Comparison of Metrics Across Models', fontsize=14, y=1.05)\n",
    "plt.savefig(\"metrics_comparison_row.png\", bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define metric names\n",
    "metrics = [\"Average Drop (%)\", \"Sparsity Score\", \"IoU (Intersection over Union)\"]\n",
    "\n",
    "# Example values for each metric (Assume these are from your experiments)\n",
    "values = [12.5, 0.75, 0.68]  # Replace with real values\n",
    "\n",
    "# Define a color palette for better visualization\n",
    "colors = ['#FF6F61', '#6B5B95', '#88B04B']\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "bars = plt.bar(metrics, values, color=colors, edgecolor='black', linewidth=1.2)\n",
    "\n",
    "# Annotate bars with values\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.02, round(yval, 2),\n",
    "             ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Labels and Title\n",
    "plt.xlabel(\"Grad-CAM Metrics\", fontsize=12, fontweight='bold')\n",
    "plt.ylabel(\"Metric Value\", fontsize=12, fontweight='bold')\n",
    "plt.title(\"Grad-CAM Evaluation Metrics for Car Dents & Scratches Detection\", fontsize=14, fontweight='bold')\n",
    "\n",
    "# Rotate x-axis labels for better visibility\n",
    "plt.xticks(rotation=20, fontsize=11)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
